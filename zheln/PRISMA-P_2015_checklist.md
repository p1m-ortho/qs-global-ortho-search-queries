# README

This document is compliant with PRISMA (2020), PRISMA-S (2020), and PRISMA-P (2015) reporting guidelines. It is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).

Please find the links to the filled-in checklists:

* [PRISMA 2020](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA_2020_checklist.md)
* [PRISMA 2020 for Abstracts](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA_2020_Abstracts_checklist.md)
* [PRISMA-S](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA-S_checklist.md)
* [PRISMA-P 2015](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA-P_2015_checklist.md)

Checklist references:

**PRISMA 2020 item checklist**

Page MJ, McKenzie J, Bossuyt P, Boutron I, Hoffmann T, Mulrow CD, Shamseer L, Tetzlaff J, Akl E, Brennan SE, Chou R, Glanville J, Grimshaw J, Hróbjartsson A, Lalu MM, Li T, Loder E, Mayo-Wilson E, McDonald S, McGuinness LA, Stewart L, Thomas J, Tricco A, Welch VA, Whiting P, Moher D. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. MetaArXiv. 2020 Sep 14. [doi: 10.31222/osf.io/v7gm2](https://doi.org/10.31222/osf.io/v7gm2).

**PRISMA-S checklist**

Rethlefsen M, Kirtley S, Waffenschmidt S, Ayala AP, Moher D, Page MJ, Koffel J. PRISMA-S: An Extension to the PRISMA Statement for Reporting Literature Searches in Systematic Reviews. Version: 5. OSF Preprints. 2020 Jul 16. [doi: 10.31219/osf.io/sfc38](https://doi.org/10.31219/osf.io/sfc38).

**PRISMA-P (Preferred Reporting Items for Systematic review and Meta-Analysis Protocols) 2015 checklist: recommended items to address in a systematic review protocol**

Shamseer L, Moher D, Clarke M, Ghersi D, Liberati A, Petticrew M, Shekelle P, Stewart LA; PRISMA-P Group. Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation. BMJ. 2015 Jan 2;350:g7647. [doi: 10.1136/bmj.g7647](https://doi.org/10.1136/bmj.g7647). Erratum in: BMJ. 2016 Jul 21;354:i4086. [PMID: 25555855](https://pubmed.gov/25555855).

> It is strongly recommended that this checklist be read in conjunction with the PRISMA-P Explanation and Elaboration (cite when available) for important clarification on the items. Amendments to a review protocol should be tracked and dated. The copyright for PRISMA-P (including checklist) is held by the PRISMA-P Group and is distributed under a Creative Commons Attribution Licence 4.0.

# ADMINISTRATIVE INFORMATION

## 1 Title

### 1a Identification: Identify the report as a protocol of a systematic review

Zheln.com: A protocol for a universal living overview of health-related systematic reviews

### 1b Update: If the protocol is for an update of a previous systematic review, identify as such

This is a protocol for a living overview of reviews, so it is updated incrementally. Its history of changes is [available from GitHub](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commits/global-sr-query/zheln/PRISMA-P_2015_checklist.md).

## 2 Registration: If registered, provide the name of the registry (such as PROSPERO) and registration number

The project is hosted on GitHub. Also, there is an [umbrella Open Science Framework project](https://doi.org/10.17605/OSF.IO/EJKFC) that mediates publication of Zheln methods between raw GitHub data (the [primary](https://github.com/drzhelnov/zheln.github.io) and [methods](https://github.com/p1m-ortho/qs-global-ortho-search-queries/tree/global-sr-query) repositories) and stable Open Science Framework preprints (project components, such as the [replicated PubMed systematic subset](https://doi.org/10.17605/OSF.IO/Z3JU7), [systematic review protocol](https://doi.org/10.17605/OSF.IO/K4Y83), [living systematic review report](https://doi.org/10.17605/OSF.IO/G6RA9)).

This protocol for an overview of systematic reviews is also about to be registered in [PROSPERO](https://www.crd.york.ac.uk/prospero/).

## 3 Authors

### 3a Contact: Provide name, institutional affiliation, e-mail address of all protocol authors; provide physical mailing address of corresponding author

Pavel Zhelnov, MD ([ORCID 0000-0003-2767-5123](http://orcid.org/0000-0003-2767-5123)). He has no institutional affiliation doing this study as an independent researcher currently based in Saint Petersburg, Russia.

34 Sobstvenny Ave, Bldg 2, Apt 105 <br>
Peterhof Saint Petersburg 198504 <br>
Russian Federation

Tel: +7 921 657 2472 <br>
Email: [pavel@zheln.com](mailto:pavel@zheln.com) <br>
Twitter: [@drzhelnov](https://twitter.com/drzhelnov)

Pavel is the only and corresponding author.

COI Statement:

> I do not have any conflict of interest.

### 3b Contributions: Describe contributions of protocol authors and identify the guarantor of the review

Pavel is the only contributor to the protocol and is the guarantor of the study.

## 4 Amendments: If the protocol represents an amendment of a previously completed or published protocol, identify as such and list changes; otherwise, state plan for documenting important protocol amendments

Any amendments to the protocol will be available from the above-mentioned repositories and will be reported in summary posts at Zheln.com.

## 5 Support

### 5a Sources: Indicate sources of financial or other support for the review

* There is no other funding for Zheln besides my private devotion and crowdfunding.
* Crowdfunding details are available from the [Zheln website](https://zheln.com).
* If Zheln videos get popular enough in the future, will also monetize these using the [YouTube Partner Program](https://support.google.com/youtube/answer/72857).

### 5b Sponsor: Provide name for the review funder and/or sponsor

Any disclosable information about crowd funders is available from the project pages on [Patreon](https://patreon.com/zheln), [GitHub Sponsors](https://github.com/sponsors/drzhelnov), and [Open Collective](https://opencollective.com/zheln).

### 5c Role of sponsor or funder: Describe roles of funder(s), sponsor(s), and/or institution(s), if any, in developing the protocol

Crowd funders had no role in the design of the protocol. They will be able to request critical appraisal and additional critical appraisal (with new data provided) of any eligible record but will not influence the review process otherwise.

# INTRODUCTION

## 6 Rationale: Describe the rationale for the review in the context of what is already known

Currently ([Papes 2020][Papes2020e13301]) and for more than 10 years now ([Bastian 2010][Bastian2010e1000326]), a wide gap has been recognized between the fast pace research is published and the limited capacity of evidence-based practitioners to promptly process it while also separating the practice-changing wheat from the chaff ([Contou 2020][Contou202070]). And, to my utmost disappointment, it is in contrast to a variety of solutions available ([Bougioukas 2020][Bougioukas2020e12318]) that the call for a dramatic change is out in 2020 ([Boutron 2020][Boutron2020135142]).

Systematic reviews represent a means of tackling the problem ([Page 2020][Page2020gwdhk]). However, they, too, are overwhelming, having given rise to _preappraised evidence summaries_ ([Haynes 2006][Haynes2006162164]). There are many resources that provide this service: [ACP Journal Club](https://www.acpjournals.org/topic/category/journal-club), [Cochrane Clinical Answers](https://www.cochranelibrary.com/cca), now legacy [DARE](https://www.crd.york.ac.uk/crdweb/ShowRecord.asp?ID=32004000332&ID=32004000332) summaries, and [L·OVE](https://iloveevidence.com) by the Epistemonikos Foundation, just to mention a few ([Chandran 2020][Chandran2020147154]).

In spite of that, I never found a resource that would have the following features at the same time (but if you did, I invite you to publicly discuss it on [Twitter](https://twitter.com/drzhelnov)):

* Exhaustive research coverage
* Focus on secondary research only
* Rigorous _conduct_ appraisals with replication attempts
* Independence proven by full process transparency
* Full Open Access
* Easy to use and friendly to general audiences

Therefore, I embarked on creating such a resource.

_History of the project:_ On [July 11, 2019](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commit/70d3bebf1ed70845950aece09122b49f58fe2880), I inititated a study with the aim of creating an exhaustive registry of systematic reviews in orthopedics (in Russian). Due to the large volume of records to screen, it was not very successful; however, it still did continue up until [Feb 17, 2020](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commit/d44e7db892a23adb8ca3eeb36cbe55b8e05b7b45), when it went into a 200-day hiatus. Since [September 6, 2020](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commit/ff1f29c87483b355b9541859d4b4c39052046936), the study has continued as [Zheln.com](https://zheln.com), a crowdfunded project.

## 7 Objectives: Provide an explicit statement of the question(s) the review will address with reference to participants, interventions, comparators, and outcomes (PICO)

PRISMA 2020 requires use of PICO or PerSPEcTiF ([Booth 2019][Booth2019e001107]) frameworks in this section. However, I could not find these relevant to this review. Therefore, I am using custom formulations:

1. Identify and monitor most of published systematic reviews.
2. Tag the identified systematic records with medical specialties.
3. Select or crowdfund reviews for further appraisal.
4. Critically appraise and replicate the selected systematic reviews.
5. Disseminate practice implications of positively appraised reviews to both the public and evidence-based practitioners.
 
Evidence-based practitioners involve healthcare practitioners, in the first place, but also practitioners in other fields associated with intervention into a human life, such as education, business, policy, or ecology.

# METHODS

## 8 Eligibility criteria: Specify the study characteristics (such as PICO, study design, setting, time frame) and report characteristics (such as years considered, language, publication status) to be used as criteria for eligibility for the review

Record eligibility is assessed by checking the record title and, if the title failed, abstract against the ‘true positive criteria’ for systematic reviews taken from the publication by [Shojania & Bero 2001][Shojania2001157162]. The publication is a diagnostic test accuracy assessment for the systematic review search strategy by the authors—the very search strategy that the search strategy of this review is based on. I quote:
  
> We regarded an article as a true positive only if the title or abstract explicitly identified the article as a systematic review or meta-analysis or if the article abstract indicated a strategy for locating the literature reviewed. Thus, an article that contained the phrase “literature review” in the title but merely stated that “relevant literature was reviewed” in the abstract would not count as a true positive. MEDLINE records without an abstract could be counted as true positives only if the title contained the words “meta-analysis,” “metaanalysis,” or “systematic review.”

There are special cases to be discussed:

* Practice guidelines as well as various review types that make use of systematic methodology (e.g., scoping reviews, realist syntheses, etc.) are not treated separately and are eligible only if the Shojania & Bero criteria are met. This is to ensure integrity of these criteria use.
* Commentaries, editorials, reprints, and other similar documents related to an original systematic review are included, as they reference a systematic review.
* Meta-analyses outside the context of a systematic review (variants on _data pooling_) are included if they are explicitly termed meta-analyses, as this is how the Shojania & Bero criteria are worded. See [Dickens 2020][Dickens2020emo0000905], for instance.
* Protocols for systematic reviews are included, as they reference a systematic review.
* Studies nesting a systematic review (e.g., case studies) are naturally included.

The record/study flow in this review is as follows:

* There is no instance when a record identified through search could be excluded from the review database, regardless of its eligibility status. However, ineligible records are not tagged and not appraised further. In contrast, all eligible records are amenable for tagging, selection, and crowdfunding process.
* Only those eligible records that have been selected or crowdfunded are subject to critical appraisal. Please find a description of the selection/crowdfunding process below.
* For all records that have been selected, all relevant reports are collected (either written by the same authors or by different authors, like in case of important referenced research). Reports are grouped into studies.
* Only for the studies appraised positively (ticked with `7. ✅ No Critical Conduct Flaws Identified by Zheln`, see below), practical implications are summarized and disseminated.

Selection/crowdfunding process:

* As a rule, I select a record for critical appraisal regardless of crowdfunding when I find the covered topic likely to have a very large practice impact with regard either to both global health care workers and the public or to the minorities. However, the decision to select is subjective and always mine to make.
* Language, date, peer-review status of the publication do not influence this decision. Also, content area of the publication (e.g., healthcare, ecology, chemistry, etc.) is also irrelevant to this decision.
* **COVID-19 publications are not selected, because there already are plenty of great resources, such as covid-nma.com, covidex.ai, cord19.vespa.ai,
WHO and Cochrane resources, just to name a few.
Also, I do not like the idea of changing universal focus of the project toward COVID-19.**
* Crowdfunding an appraisal of any eligible record is possible for any individual or organization. Such records are marked with different icons but are otherwise treated in the same way as the selected ones.

## 9 Information sources: Describe all intended information sources (such as electronic databases, contact with study authors, trial registers or other grey literature sources) with planned dates of coverage

Please see the [PRISMA-S checklist](#prisma-s-checklist) below.

## 10 Search strategy: Present draft of search strategy to be used for at least one electronic database, including planned limits, such that it could be repeated

Please see the [PRISMA-S checklist](#prisma-s-checklist) below.

## 11 Study records

### 11a Data management: Describe the mechanism(s) that will be used to manage records and data throughout the review

All raw records and data are publicly available from the project [primary](https://github.com/drzhelnov/zheln.github.io) and [methods](https://github.com/p1m-ortho/qs-global-ortho-search-queries/tree/global-sr-query) repositories:

* [Raw search query results](https://github.com/p1m-ortho/qs-global-ortho-search-queries/tree/global-sr-query/zheln/summary-systematic-set)
* [Guidance on record generation](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/README.md#appendix-how-to-compile-zheln-records)
* [Editable records](https://github.com/p1m-ortho/qs-global-ortho-search-queries/tree/global-sr-query/zheln/posts-edit)
* [Appraisal log](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/Appraisal_Log.md)
* [PRISMA 2020 flow diagrams](https://github.com/drzhelnov/zheln.github.io/tree/master/flow-diagram)

All processed records are available from the [Zheln website](https://zheln.com). Operational information on how this website works and how to recreate the research environment in use are also available from these repositories.

In addition to all review data being publicly available, the project management information is also publicly available:

* Zheln uses public [GitHub kanban boards](https://docs.github.com/en/github/managing-your-work-on-github/about-project-boards) to manage its workflow.
* Namely, important tasks needed to accomplish the [Zheln’s mission](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/README.md#zhelns-mission) are created as public Projects on the [Zheln GitHub page](https://github.com/drzhelnov/zheln.github.io/projects), e.g., [Streamline Zheln](https://github.com/drzhelnov/zheln.github.io/projects/1) or [Register with PROSPERO](https://github.com/drzhelnov/zheln.github.io/projects/2).
* Further, these tasks are broken down into smaller blocks ([GitHub Issues](https://github.com/drzhelnov/zheln.github.io/issues)) and are displayed on the projects’ kanban boards.
* Finally, each week, short-term [GitHub Milestones](https://github.com/drzhelnov/zheln.github.io/milestones) are created and then assigned to the Issues to track weekly progress.
* All of these elements are public. Issues are open for public comment (GitHub registration needed).
* To maximize publicity, I additionally endeavor to publish [summary posts](https://zheln.com) twice a week, where I overview the events that happened on Zheln since the last summary and that I consider important.

### 11b Selection process: State the process that will be used for selecting studies (such as two independent reviewers) through each phase of the review (that is, screening, eligibility and inclusion in meta-analysis)

Please see the [8 Eligibility criteria](#8-eligibility-criteria-specify-the-study-characteristics-such-as-pico-study-design-setting-time-frame-and-report-characteristics-such-as-years-considered-language-publication-status-to-be-used-as-criteria-for-eligibility-for-the-review) section above.

Currently, I am the only person who undertakes the whole selection process. In the future, when more staff is available, other people will get involved; this will be additionally reported.

Likewise, no automation tools or crowdsourcing are currently used in the selection process. They are considered for future use; this will be additionally reported.

### 11c Data collection process: Describe planned method of extracting data from reports (such as piloting forms, done independently, in duplicate), any processes for obtaining and confirming data from investigators

Currently, I am the only person who undertakes the whole data collection process. In the future, when more staff is available, other people will get involved; this will be additionally reported.

Tagging of records:

* Specialty-tagging is done by the appraiser themselves based on whatever information they acquired during record screening.
* Specialty tags may also be assigned or removed throughout further full appraisal if this is deemed appropriate to better reflect the content of the record.
* The tags are chosen from [191 specialty tags](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/zheln_ama_specialty_tags.lst) made from a [list of 171 AMA Masterfile Physician Specialties](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/zheln_ama_specialty_tags_config.lst). I have chosen this specialty classification because it is, arguably, the most elaborate one (in terms of listed subspecialties) among those widely used. See the methods used to compile the lists of specialties and specialty tags in the [commit history of the Zheln methods repository](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commits/global-sr-query).
* While tagging, the tagger should consider if the record would be accessible from its most relevant [specialty page](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/zheln_ama_specialty_page_filenames.lst) and ensure it would. Namely, the tagger must check the record against **each** specialty on the list, assess if this record could be of interest to a typical physician working in this specialty, and choose one specialty that looks most relevant in this regard.
* If choosing one such specialty is hard enough, choosing more than one is acceptable. As such, adding more than one specialty tag is actively discouraged unless absolutely unavoidable, as it remarkably compromises appraisal speed. Therefore, multiple specialties are warranted if and only if it is perceived to be faster to multiselect than choose one among several.
* I do recognize the best approach to specialty-grouping would be an evidence-based approach involving feedback from the physicians themselves. However, this is unavailable at the moment, so specialty-grouping will be theory-based. In the future, introducing empirical testing will be of interest.
* If the tagger is uncertain either of the article subject or any specialty scope, they should consult Google or other information sources until both seem perfectly clear.
* There is no limit to the number of specialty tags attached, but at least one tag should be chosen for each record. However, it became evident during pilot appraisals that some records on PubMed cover topics rather distant from health care. For these records, as an exception, there may be no specialty tags added when each and every specialty tag clearly does not apply.

* I do not plan to add any new specialty tags. However, if I hear about any changes to the AMA Masterfile Physician Specialties list, I will consider updating the Zheln specialty lists accordingly.

Collection of reports:

* I will try to collect the original full text and all other relevant reports associated with the record under review, if applicable, either using publicly available electronic resources or via private subscriptions or communication.
* If no full-text report is available for the record, I will put the `3. ❌ No Full Text Available to Zheln` label, place the record in the `awaiting crowdfunding` category, and reference it in the **Full Text Wanted** section of twice-weekly summary posts at Zheln.com. When new reports become available, I will go on with the critical appraisal.
* I will not routinely contact the authors or search extensively for additional reports, but if the study is somehow specifically important or additional appraisal is crowdfunded, will do that.

Data extraction from reports:

* Involves extracting the pragmatic outcomes that I deemed relevant for evidence-based practice. This is done only if the study has been positively appraised (ticked with `7. ✅ No Critical Conduct Flaws Identified by Zheln`). If so, I will extract the data directly in the text on the record page at Zheln.com.
* I will decide what outcomes to extract depending on the research question. As a rule, I will choose one among all outcomes mentioned in the study reports and will contrast it to the outcomes usually used in similar studies to decide if this is an acceptably practice-important and question-relevant outcome.
* To learn what outcomes are best suited for similar research questions, I am planning to use informal electronic search (Google, PubMed, etc.), that I am not planning to document, but I will summarize important information on this matter, including references, if applicable, in the text of the appraisal.
* No automation tools are currently used. If any are in the future, this will be additionally reported.
* Translation of reports written in languages other than English will usually be handled using [Google Translate](https://translate.google.com). In any way, this will be reported in the text on the record page at Zheln.com.
* Any inconsistencies met when collating multiple reports will be discussed and resolved narratively in the text on the record page at Zheln.com.
* If I did find an appropriate outcome in the study reports, I will use it as a _primary outcome_ to then assess effectiveness of the interventions. Otherwise, if I did not find any appropriate outcomes reported, I will mark the record as `4. ❌ Does Not Generate Pragmatic Evidence Directly Relevant to Evidence-Based Practice` and abstain from further appraisal.
* Eventually, I will formulate explicit practice-relevant statements based on the extracted health outcomes and quality-of-conduct assessment (see below). It will be done directly in the text on the record page at Zheln.com, stressed, and placed at its top.
* Also, the process of each critical appraisal is video-recorded and published on [YouTube](https://www.youtube.com/channel/UCMNQzA3-71TyD-fVbXnxfKQ) to disseminate this information among general audiences. One video-appraisal is uploaded [every working day](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/Work_Schedule.md).

## 12 Data items: List and define all variables for which data will be sought (such as PICO items, funding sources), any pre-planned data assumptions and simplifications

Unfortunately, due to universal nature of the review, data items cannot be predefined. However, I will discuss all issues and report all decisions for each appraisal separately in the text on their record page at Zheln.com.

Please also see [11c Data collection process](#11cdata-collection-process-describe-planned-method-of-extracting-data-from-reports-such-as-piloting-forms-done-independently-in-duplicate-any-processes-for-obtaining-and-confirming-data-from-investigators).

## 13 Outcomes and prioritization: List and define all outcomes for which data will be sought, including prioritization of main and additional outcomes, with rationale

Please see [12 Data items](#12-data-items-list-and-define-all-variables-for-which-data-will-be-sought-such-as-pico-items-funding-sources-any-pre-planned-data-assumptions-and-simplifications).

## 14 Risk of bias in individual studies: Describe anticipated methods for assessing risk of bias of individual studies, including whether this will be done at the outcome or study level, or both; state how this information will be used in data synthesis

Currently, I am the only person who undertakes the whole risk-of-bias assessment process. In the future, when more staff is available, other people will get involved; this will be additionally reported. Likewise, no automation tools are currently in use.

The methods of critical appraisal may be described in one phrase:
  
Informal study of the review documentation with replication of some of the elements of the review by a single appraiser to formulate their expert impression as to whether the review is reproducible and whether it is useful for evidence-based practice.

Please find by-step description below.

Assessing duplication:

* To find out, I am planning to use informal electronic search (Google, PubMed, etc.), that I am not planning to document, but I will summarize important information on this matter, including references, if applicable, in the text on the record page at Zheln.com.
* If I find evidence the study is duplicate (same population, same context, same interventions, same outcomes, with no reasonable reference to previous research), I should naturally not start answering the research question at this study but use previous research first. Therefore, I will abstain from appraising duplicate studies and will mark them as `5. ❌ Is Duplicate`.
* Otherwise, I will mark the study appropriately as `5. ✅ Not Found Duplicate by Zheln` and go on with the appraisal.

Replication:

* If a systematic review features practice-important outcomes _and_ does not look duplicate, it feels safe and appropriate to embark on its replication.
* In the course of this review, I am not planning to conduct exhaustive replications. In contrast, I am going to try and replicate selectively those review steps that look both easiest and most natural to redo.
* For example, rerunning PubMed/MEDLINE searches and replicating the review study set is usually simple enough and appealing, as is reproducing a random couple of data extraction forms. I will document these replication processes in the text on the record page at Zheln.com.
* If I deem the replication attempts more or less successful, I will label the study as `6. ✅ Passed Replication`.
* Otherwise, if the replication has largely failed, I will mark the study as `6. ❌ Failed Replication` and abstain from further appraisal because an irreproducible review is hardly systematic anymore.

Quality-of-conduct assessment:

* Replicability is a sign of sound conduct and good reporting but does not guarantee robustness of the review. Therefore, additional quality-of-conduct assessment is required.
* Some tools have been developed to assess risk of bias in systematic reviews, such as [ROBIS](https://www.bristol.ac.uk/population-health-sciences/projects/robis) or [CINeMA](https://doi.org/10.1371/journal.pmed.1003082). However, they are rather recent, and there is evidence agreement is low at least for some of them ([Gates 2020][Gates2020915]).
* In contrast, [MECIR](https://community.cochrane.org/mecir-manual) have been out there for quite some time now. Also, they have been neatly integrated into [Cochrane Handbook 6](https://training.cochrane.org/handbook/current) that provides further insight into these issues.
* Thus, I elect to use the [MECIR conduct standards](https://community.cochrane.org/mecir-manual/standards-conduct-new-cochrane-intervention-reviews-c1-c75) to assess quality of conduct. I will go over all 75 MECIR conduct items to get an understanding and will document for each item if it was followed, in my view. I will also provide rationale where relevant. All the documentation will take place directly in the text on the record page at Zheln.com.
* In general, I expect _mandatory_ MECIR items to be followed, whereas _highly desirable_ items may be ignored. However, I acknowledge that MECIR standards (1) are not absolute and (2) were developed for intervention reviews only (whereas this overview may feature other systematic reviews as well). Also, some of the items would look critical for one review and not critical for another.
* Hence, the final decision about whether or not I have observed evidence of critical conduct flaws is always mine to make; at the same time, I will do my best to accurately substantiate my findings in the appraisal text. My decision may be `7. ❌ Has Critical Conduct Flaws`, `7. ✅ No Critical Conduct Flaws Identified by Zheln`, or `7. ✅ No Conduct Flaws Identified by Zheln`, depending on those findings.

I am not planning to conduct any assessment of quality of reporting.

Also, I will note this as a solely subjective item: `8. 👍 Liked by Zheln` if I find the review useful overall and `8. 👎 Disliked by Zheln` if I find otherwise. I will provide any personal comment if my decision did not seem obvious.

## 15 Data synthesis

### 15a Describe criteria under which study data will be quantitatively synthesised

No across-studies synthesis is planned at the moment. If any is conducted, it will be reported separately for each study in the text on the record page at Zheln.com.

### 15b If data are appropriate for quantitative synthesis, describe planned summary measures, methods of handling data and methods of combining data from studies, including any planned exploration of consistency (such as I2, Kendall’s τ)

Please see [15a Describe criteria under which study data will be quantitatively synthesised](#15a-describe-criteria-under-which-study-data-will-be-quantitatively-synthesised).

### 15c Describe any proposed additional analyses (such as sensitivity or subgroup analyses, meta-regression)

Please see [15a Describe criteria under which study data will be quantitatively synthesised](#15a-describe-criteria-under-which-study-data-will-be-quantitatively-synthesised).

### 15d If quantitative synthesis is not appropriate, describe the type of summary planned

Please see [15a Describe criteria under which study data will be quantitatively synthesised](#15a-describe-criteria-under-which-study-data-will-be-quantitatively-synthesised).

## 16 Meta-bias(es): Specify any planned assessment of meta-bias(es) (such as publication bias across studies, selective reporting within studies)

* I will use the preliminary version of the [ROB-ME](https://www.riskofbias.info/welcome/rob-me-tool) tool for reporting bias assessment. All documentation will take place directly in the text on the record page at Zheln.com. However, I do not have in mind any generic algorithm to assess overall risk of bias, so I will elaborate on it for each appraisal separately.
* I will be the only assessor for now.
* No automation tools will be used.
* The authors will not be contacted routinely. However, if specifically requested when crowdfunded or at will, will be done.

## 17 Confidence in cumulative evidence: Describe how the strength of the body of evidence will be assessed (such as GRADE)

* I will use the [GRADE](https://gdt.gradepro.org/app/handbook/handbook.html) tool in its interpretation for systematic review authors. All documentation will take place directly in the text on the record page at Zheln.com. I do not have in mind any generic algorithm to assess certainty of evidence, so I will elaborate on it for each appraisal separately.
* I will be the only assessor for now.
* No automation tools will be used.
* The authors will not be contacted routinely. However, if specifically requested when crowdfunded or at will, will be done.

# PRISMA-S checklist

# INFORMATION SOURCES AND METHODS

## 1 Database name: Name each individual database searched, stating the platform for each

MEDLINE via PubMed (https://pubmed.ncbi.nlm.nih.gov)

Adding other search sources, such as Scopus, OSF, and medRxiv, is planned in the future when more appraisers become available.

## 2 Multi-database searching: If databases were searched simultaneously on a single platform, state the name of the platform, listing all of the databases searched

Not used yet.

## 3 Study registries: List any study registries searched

Not used yet.

Adding other search sources, such as PROSPERO and OSF Registries, is planned in the future when more appraisers become available.

## 4 Online resources and browsing: Describe any online or print source purposefully searched or browsed (e.g., tables of contents, printconference proceedings, web sites), and how this was done

Not used yet.

## 5 Citation searching: Indicate whether cited references or citing references were examined, and describe any methods used for locating cited/citing references (e.g., browsing reference lists, using a citation index, setting up email alerts for references citing included studies)

Not used yet.

If relevant references are somehow identified _during_ critical appraisal, their associated reports will be collected and added to the total report count.

## 6 Contacts: Indicate whether additional studies or data were sought by contacting authors, experts, manufacturers, or others

Will not be done routinely. However, if specifically requested when crowdfunded or at will, will be done.

## 7 Other methods: Describe any additional information sources or search methods used

None used.

# SEARCH STRATEGIES

## 8 Full search strategies: Include the search strategies for each database and information source, copied and pasted exactly as run

* This is the Replicated Version of the _PubMed Systematic Review Subset Query, Zheln Edition_.
* Before use, replace all the upper limit dates with the date needed using any text editor.
* If everything done correctly and the query itself still works, you will get a consistent set of records each time on whatever date you run the query.

```
(
    (
        (
            (
                (
                    (
                        "systematic review"[ti] OR "systematic literature review"[ti] OR "systematic scoping review"[ti] OR "systematic narrative review"[ti] OR "systematic qualitative review"[ti] OR "systematic evidence review"[ti] OR "systematic quantitative review"[ti] OR "systematic meta review"[ti] OR "systematic critical review"[ti] OR "systematic mixed studies review"[ti] OR "systematic mapping review"[ti] OR "systematic cochrane review"[ti] OR "systematic search and review"[ti] OR "systematic integrative review"[ti]
                    )
                    AND
                    1865/01/01:2020/09/09[crdt]
                )
                NOT
                (
                    "comment"[pt]
                    AND
                    1865/01/01:2020/09/09[dcom]
                )
                NOT
                (
                    (
                        "protocol"[ti] OR "protocols"[ti]
                    )
                    AND
                    1865/01/01:2020/09/09[crdt]
                )
            )
            NOT
            (
                "medline"[sb]
                AND
                1865/01/01:2020/09/09[dcom]
            )
        )
        OR
        (
            (
                "cochrane database syst rev"[ta]
                AND
                1865/01/01:2020/09/09[crdt]
            )
            AND
            (
                "review"[pt]
                AND
                1865/01/01:2020/09/09[dcom]
            )
        )
        OR
        (
            "systematic review"[pt]
            AND
            1865/01/01:2020/09/09[dcom]
        )
    )
    OR
    (
        (
            (
                "systematic review"[ti]
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                "meta-analysis"[pt]
                AND
                1865/01/01:2020/09/09[dcom]
            )
            OR
            (
                (
                    "meta analysis"[ti] OR "systematic literature review"[ti]
                )
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                (
                    "this systematic review"[tw] OR "pooling project"[tw]
                )
                AND
                (
                    1865/01/01:2020/09/09[dcom]
                    OR
                    1865/01/01:2020/09/09[mhda]
                )
            )
            OR
            (
                (
                    "this systematic review"[tiab] OR "pooling project"[tiab]
                )
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                (
                    "systematic review"[tiab]
                    AND
                    1865/01/01:2020/09/09[crdt]
                )
                AND
                (
                    "review"[pt]
                    AND
                    1865/01/01:2020/09/09[dcom]
                )
            )
            OR
            (
                (
                    "meta synthesis"[ti] OR "meta analy*"[ti]
                )
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                (
                    "integrative review"[tw] OR "integrative research review"[tw] OR "rapid review"[tw] OR "umbrella review"[tw]
                )
                AND
                (
                    1865/01/01:2020/09/09[dcom]
                    OR
                    1865/01/01:2020/09/09[mhda]
                )
            )
            OR
            (
                (
                    "integrative review"[tiab] OR "integrative research review"[tiab] OR "rapid review"[tiab] OR "umbrella review"[tiab]
                )
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                (
                    "consensus development conference"[pt] OR "practice guideline"[pt]
                )
                AND
                1865/01/01:2020/09/09[dcom]
            )
            OR
            (
                (
                    "drug class reviews"[ti] OR "cochrane database syst rev"[ta] OR "acp journal club"[ta] OR "health technol assess"[ta] OR "evid rep technol assess summ"[ta] OR "jbi database system rev implement rep"[ta]
                )
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                "clinical guideline"[tw]
                AND
                "management"[tw]
                AND
                (
                    1865/01/01:2020/09/09[dcom]
                    OR
                    1865/01/01:2020/09/09[mhda]
                )
            )
            OR
            (
                "clinical guideline"[tiab]
                AND
                "management"[tiab]
                AND
                1865/01/01:2020/09/09[crdt]
            )
            OR
            (
                (
                    (
                        "evidence based"[ti]
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        "evidence-based medicine"[mh]
                        AND
                        1865/01/01:2020/09/09[mhda]
                    )
                    OR
                    (
                        (
                            "best practice*"[ti] OR "evidence synthesis"[tiab]
                        )
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                )
                AND
                (
                    (
                        (
                            "review"[pt] OR "evaluation study"[pt] OR "validation study"[pt] OR "guideline"[pt] OR "pmcbook"[all]
                        )
                        AND
                        1865/01/01:2020/09/09[dcom]
                    )
                    OR
                    (
                        (
                            "diseases category"[mh] OR "behavior and behavior mechanisms"[mh] OR "therapeutics"[mh]
                        )
                        AND
                        1865/01/01:2020/09/09[mhda]
                    )
                )
            )
            OR
            (
                (
                    (
                        (
                            "systematic"[tw] OR "systematically"[tw] OR "study selection"[tw]
                            OR
                            (
                                (
                                    "predetermined"[tw] OR "inclusion"[tw]
                                )
                                AND
                                "criteri*"[tw]
                            )
                            OR
                            "exclusion criteri*"[tw] OR "main outcome measures"[tw] OR "standard of care"[tw] OR "standards of care"[tw]
                        )
                        AND
                        (
                            1865/01/01:2020/09/09[dcom]
                            OR
                            1865/01/01:2020/09/09[mhda]
                        )
                    )
                    OR
                    (
                        (
                            "systematic"[tiab] OR "systematically"[tiab] OR "study selection"[tiab]
                            OR
                            (
                                (
                                    "predetermined"[tiab] OR "inclusion"[tiab]
                                )
                                AND
                                "criteri*"[tiab]
                            )
                            OR
                            "exclusion criteri*"[tiab] OR "main outcome measures"[tiab] OR "standard of care"[tiab] OR "standards of care"[tiab]
                        )
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        "critical"[tiab] 
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                )
                AND
                (
                    (
                        (
                            "survey"[tiab] OR "surveys"[tiab] OR "review"[tiab] OR "reviews"[tiab] OR "analysis"[ti] OR "critique"[tiab]
                        )
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        (
                            "overview*"[tw] OR "search*"[tw] OR "handsearch"[tw] OR "appraisal"[tw]
                        )
                        AND
                        (
                            1865/01/01:2020/09/09[dcom]
                            OR
                            1865/01/01:2020/09/09[mhda]
                        )
                    )
                    OR
                    (
                        (
                            "overview*"[tiab] OR "search*"[tiab] OR "handsearch"[tiab] OR "appraisal"[tiab]
                        )
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        (
                            (
                                "reduction"[tw]
                                AND
                                (
                                    1865/01/01:2020/09/09[dcom]
                                    OR
                                    1865/01/01:2020/09/09[mhda]
                                )
                            )
                            OR
                            (
                                "reduction"[tiab]
                                AND
                                1865/01/01:2020/09/09[crdt]
                            )
                        )
                        AND
                        (
                            (
                                "risk"[mh]
                                AND
                                1865/01/01:2020/09/09[mhda]
                            )
                            OR
                            (
                                (
                                    "risk"[tw]
                                    AND
                                    (
                                        1865/01/01:2020/09/09[dcom]
                                        OR
                                        1865/01/01:2020/09/09[mhda]
                                    )
                                )
                                OR
                                (
                                    "risk"[tiab]
                                    AND
                                    1865/01/01:2020/09/09[crdt]
                                )
                            )
                        )
                        AND
                        (
                            (
                                (
                                    "death"[mh] OR "recurrence"[mh]
                                )
                                AND
                                1865/01/01:2020/09/09[mhda]
                            )
                            OR
                            (
                                (
                                    "death"[all] OR "recurrence"[all]
                                )
                                AND
                                1865/01/01:2020/09/09[dcom]
                            )
                        )
                    )
                )
                AND
                (
                    (
                        (
                            "literature"[tiab] OR "articles"[tiab] OR "publications"[tiab] OR "publication"[tiab] OR "bibliography"[tiab] OR "bibliographies"[tiab] OR "published"[tiab] OR "database"[tiab] OR "internet"[tiab] OR "textbooks"[tiab] OR "trials"[tiab]
                        )
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        (
                            "pooled data"[tw] OR "unpublished"[tw] OR "citation"[tw] OR "citations"[tw] OR "references"[tw] OR "scales"[tw] OR "papers"[tw] OR "datasets"[tw] OR "meta analy*"[tw]
                        )
                        AND
                        (
                            1865/01/01:2020/09/09[dcom]
                            OR
                            1865/01/01:2020/09/09[mhda]
                        )
                    )
                    OR
                    (
                        (
                            "pooled data"[tiab] OR "unpublished"[tiab] OR "citation"[tiab] OR "citations"[tiab] OR "references"[tiab] OR "scales"[tiab] OR "papers"[tiab] OR "datasets"[tiab] OR "meta analy*"[tiab]
                        )
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        "clinical"[tiab]
                        AND
                        "studies"[tiab]
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        "treatment outcome"[mh]
                        AND
                        1865/01/01:2020/09/09[mhda]
                    )
                    OR
                    (
                        "treatment outcome"[tw]
                        AND
                        (
                            1865/01/01:2020/09/09[dcom]
                            OR
                            1865/01/01:2020/09/09[mhda]
                        )
                    )
                    OR
                    (
                        "treatment outcome"[tiab]
                        AND
                        1865/01/01:2020/09/09[crdt]
                    )
                    OR
                    (
                        "pmcbook"[all]
                        AND
                        1865/01/01:2020/09/09[dcom]
                    )
                )
            )
        )
        NOT
        (
            (
                "letter"[pt] OR "newspaper article"[pt]
            )
            AND
            1865/01/01:2020/09/09[dcom]
        )
    )
)
AND
(2020/09/09:2020/09/09[crdt] OR 2020/09/09:2020/09/09[dcom] OR 2020/09/09:2020/09/09[mhda])
```

## 9 Limits and restrictions: Specify that no limits were used, or describe any limits or restrictions applied to a search (e.g., date or time period, language, study design) and provide justification for their use

None used except those built in the query.

## 10 Search filters: Indicate whether published search filters were used (as originally designed or modified), and if so, cite the filter(s) used

* The _PubMed Systematic Review Subset Query, Zheln Edition,_ ultimately represents the [Search Strategy Used to Create the Systematic Reviews Subset on PubMed](https://www.nlm.nih.gov/bsd/pubmed_subsets/sysreviews_strategy.html) as joined from the two versions: [February 2017](http://web.archive.org/web/20181023065423/https://www.nlm.nih.gov/bsd/pubmed_subsets/sysreviews_strategy.html) and [December 2018](http://web.archive.org/web/20190711085949/https://www.nlm.nih.gov/bsd/pubmed_subsets/sysreviews_strategy.html).
* The February 2017 version is [based](https://wayback.archive-it.org/org-350/20180406175620/https://www.nlm.nih.gov/bsd/pubmed_subsets/sysreviews_sources.html) on the study by [Shojania & Bero 2001][Shojania2001157162]. I do not know what the December 2018 version is based on, as it has not been written there.
* See a detailed account of the development of the _Zheln Edition_ and its testing reports in the [commit history of the Zheln methods repository](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commits/global-sr-query). While lurking through the commit history, be sure to inspect [both the diff _and_ the body of the commits](https://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository). This information is also going to be republished on a [dedicated Open Science Framework page](https://doi.org/10.17605/OSF.IO/Z3JU7).

## 11 Prior work: Indicate when search strategies from other literature reviews were adapted or reused for a substantive part or all of the search, citing the previous review(s)

None used other than already mentioned.

## 12 Updates: Report the methods used to update the search(es) (e.g., rerunning searches, email alerts)

Manual reruns through the website interface.

Implementing automatic updates using PubMed APIs is considered for the future when more staff is available.

## 13 Dates of searches: For each search strategy, provide the date when the last search occurred

The searches are run daily, please see the [directory containing searches in the Zheln methods repository](https://github.com/p1m-ortho/qs-global-ortho-search-queries/tree/global-sr-query/zheln/ssb-replicated-ae408b9).

# PEER REVIEW

## 14 Peer review: Describe any search peer review process

None.

# MANAGING RECORDS

## 15 Total records: Document the total number of records identified from each database and other information sources

Please consult the [Appraisal Log](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/Appraisal_Log.md).

Also, this information is available from flow diagrams in the latest summary posts at the [Zheln website](https://zheln.com).

## 16 Deduplication: Describe the processes and any software used to deduplicate records from multiple database searches and other information sources

None done, because records are reviewed separately for each date and are unlikely to be duplicate within a single PubMed search.

[Page2020gwdhk]: https://doi.org/10.31222/osf.io/gwdhk "Page MJ, Moher D, Bossuyt P, Boutron I, Hoffmann T, Mulrow CD, Shamseer L, Tetzlaff J, Akl E, Brennan SE, Chou R, Glanville J, Grimshaw J, Hróbjartsson A, Lalu MM, Li T, Loder E, Mayo-Wilson E, McDonald S, McGuinness LA, Stewart L, Thomas J, Tricco A, Welch VA, Whiting P, McKenzie J. PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. MetaArXiv. 2020 Sep 14. doi: 10.31222/osf.io/gwdhk"

[Papes2020e13301]: https://doi.org/10.1111/eci.13301 "Papes D, Jeroncic A, Ozimec E. Redundancy and methodological issues in articles on COVID-19. Eur J Clin Invest. 2020 Jun 7:e13301. doi: 10.1111/eci.13301. Epub ahead of print. PMID: 32506512; PMCID: PMC7300618."

[Bastian2010e1000326]: https://doi.org/10.1371/journal.pmed.1000326 "Bastian H, Glasziou P, Chalmers I. Seventy-five trials and eleven systematic reviews a day: how will we ever keep up? PLoS Med. 2010 Sep 21;7(9):e1000326. doi: 10.1371/journal.pmed.1000326. PMID: 20877712; PMCID: PMC2943439."

[Contou202070]: https://doi.org/10.1186/s13613-020-00688-2 "Contou D, Thirion M, Pajot O, Plantefève G, Mentec H. Journal club in an ICU: rate and factors associated with practice-changing articles. Analysis of 1712 articles read over a 13-year period (2007-2019). Ann Intensive Care. 2020 Jun 1;10(1):70. doi: 10.1186/s13613-020-00688-2. PMID: 32488529; PMCID: PMC7266895."

[Bougioukas2020e12318]: https://doi.org/10.1111/hir.12318 "Bougioukas KI, Bouras EC, Avgerinos KI, Dardavessis T, Haidich AB. How to keep up to date with medical information using web-based resources: a systematised review and narrative synthesis. Health Info Libr J. 2020 Jul 21. doi: 10.1111/hir.12318. Epub ahead of print. PMID: 32691960."

[Boutron2020135142]: https://doi.org/10.1016/j.jclinepi.2020.01.024 "Boutron I, Créquit P, Williams H, Meerpohl J, Craig JC, Ravaud P. Future of evidence ecosystem series: 1. Introduction Evidence synthesis ecosystem needs dramatic change. J Clin Epidemiol. 2020 Jul;123:135-142. doi: 10.1016/j.jclinepi.2020.01.024. Epub 2020 Mar 4. PMID: 32145367."

[Haynes2006162164]: https://doi.org/10.1136/ebm.11.6.162-a "Haynes RB. Of studies, syntheses, synopses, summaries, and systems: the “5S” evolution of information services for evidence-based health care decisions. ACP J Club. 2006 Nov-Dec;145(3):A8. doi: 10.1136/ebm.11.6.162-a. PMID: 17080967."

[Chandran2020147154]: https://doi.org/10.7324/JAPS.2020.10717 "Chandran PV, Khan S, Pai KG, Khera K, Devi ES, Athira B, Thunga G. Evidence-based medicine databases: an overview. J Appl Pharm Sci. 2020 Jul;10(7):147-154. doi: 10.7324/JAPS.2020.10717."

[Booth2019e001107]: https://doi.org/10.1136/bmjgh-2018-001107 "Booth A, Noyes J, Flemming K, Moore G, Tunçalp Ö, Shakibazadeh E. Formulating questions to explore complex interventions within qualitative evidence synthesis. BMJ Glob Health. 2019 Jan 25;4(Suppl 1):e001107. doi: 10.1136/bmjgh-2018-001107. PMID: 30775019; PMCID: PMC6350737."

[Shojania2001157162]: https://www.researchgate.net/publication/11820967_Taking_Advantage_of_the_Explosion_of_Systematic_Reviews_An_Efficient_MEDLINE_Search_Strategy "Shojania KG, Bero LA. Taking advantage of the explosion of systematic reviews: an efficient MEDLINE search strategy. Eff Clin Pract. 2001 Jul-Aug;4(4):157-62. PMID: 11525102."

[Dickens2020emo0000905]: https://doi.org/10.1037/emo0000905 "Dickens LR, Robins RW. Pride: A meta-analytic project. Emotion. 2020 Nov 12. doi: 10.1037/emo0000905. Epub ahead of print. PMID: 33180528."

[Gates2020915]: https://doi.org/10.1016/j.jclinepi.2020.04.026 "Gates M, Gates A, Duarte G, Cary M, Becker M, Prediger B, Vandermeer B, Fernandes RM, Pieper D, Hartling L. Quality and risk of bias appraisals of systematic reviews are inconsistent across reviewers and centers. J Clin Epidemiol. 2020 Sep;125:9-15. doi: 10.1016/j.jclinepi.2020.04.026. Epub 2020 May 19. PMID: 32416337."
