# README

**PRISMA 2020 item checklist**

Page MJ, McKenzie J, Bossuyt P, Boutron I, Hoffmann T, Mulrow CD, Shamseer L, Tetzlaff J, Akl E, Brennan SE, Chou R, Glanville J, Grimshaw J, Hróbjartsson A, Lalu MM, Li T, Loder E, Mayo-Wilson E, McDonald S, McGuinness LA, Stewart L, Thomas J, Tricco A, Welch VA, Whiting P, Moher D. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. MetaArXiv. 2020 Sep 14. [doi: 10.31222/osf.io/v7gm2](https://doi.org/10.31222/osf.io/v7gm2).

# TITLE

## 1 Title: Identify the report as a systematic review

Zheln.com: A protocol for a universal living overview of health-related systematic reviews

# ABSTRACT

## 2 Abstract: See the PRISMA 2020 for Abstracts checklist (Table 2)

[PRISMA 2020 for Abstracts checklist](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA_2020_Abstracts_checklist.md)

# INTRODUCTION

## 3 Rationale: Describe the rationale for the review in the context of existing knowledge

Currently ([Papes 2020][Papes2020e13301]) and for more than 10 years now ([Bastian 2010][Bastian2010e1000326]), a wide gap has been recognized between the fast pace research is published and the limited capacity of evidence-based practitioners to promptly process it while also separating the practice-changing wheat from the chaff ([Contou 2020][Contou202070]). And, to my utmost disappointment, it is in contrast to a variety of solutions available ([Bougioukas 2020][Bougioukas2020e12318]) that the call for a dramatic change is out in 2020 ([Boutron 2020][Boutron2020135142]).

Systematic reviews represent a means of tackling the problem ([Page 2020][Page2020gwdhk]). However, they, too, are overwhelming, having given rise to _preappraised evidence summaries_ ([Haynes 2006][Haynes2006162164]). There are many resources that provide this service: [ACP Journal Club](https://www.acpjournals.org/topic/category/journal-club), [Cochrane Clinical Answers](https://www.cochranelibrary.com/cca), now legacy [DARE](https://www.crd.york.ac.uk/crdweb/ShowRecord.asp?ID=32004000332&ID=32004000332) summaries, and [L·OVE](https://iloveevidence.com) by the Epistemonikos Foundation, just to mention a few ([Chandran 2020][Chandran2020147154]).

In spite of that, I never found a resource that would have the following features at the same time (but if you did, I invite you to publicly discuss it on [Twitter](https://twitter.com/drzhelnov)):

* Exhaustive research coverage
* Focus on secondary research only
* Rigorous _conduct_ appraisals with replication attempts
* Independence proven by full process transparency
* Full Open Access
* Easy to use and friendly to general audiences

Therefore, I embarked on creating such a resource.

_History of the project:_ On [July 11, 2019](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commit/70d3bebf1ed70845950aece09122b49f58fe2880), I inititated a study with the aim of creating an exhaustive registry of systematic reviews in orthopedics (in Russian). Due to the large volume of records to screen, it was not very successful; however, it still did continue up until [Feb 17, 2020](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commit/d44e7db892a23adb8ca3eeb36cbe55b8e05b7b45), when it went into a 200-day hiatus. Since [September 6, 2020](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commit/ff1f29c87483b355b9541859d4b4c39052046936), the study has continued as [![Woodpecker by Anton from The Noun Project](https://raw.githubusercontent.com/drzhelnov/zheln.github.io/gh-pages/favicons/favicon-16x16.png)](https://zheln.com) [Zheln.com](https://zheln.com), a crowdfunding project.

## 4 Objectives: Provide an explicit statement of the objective(s) or question(s) the review addresses

PRISMA 2020 requires use of PICO or PerSPEcTiF ([Booth 2019][Booth2019e001107]) frameworks in this section. However, I could not find these relevant to this review. Therefore, I am using custom formulations:

1. Identify and monitor most of published systematic reviews.
2. Tag the identified systematic records with medical specialties.
3. Select or crowdfund reviews for further appraisal.
4. Critically appraise and replicate the selected systematic reviews.
5. Disseminate practice implications of positively appraised reviews to both the public and evidence-based practitioners.
 
Evidence-based practitioners involve healthcare practitioners, in the first place, but also practitioners in other fields associated with intervention into a human life, such as education, business, policy, or ecology.

# METHODS

## 5 Eligibility criteria: Specify the inclusion and exclusion criteria for the review and how studies were grouped for the syntheses

Record eligibility is assessed by checking the record title and, if the title failed, abstract against the ‘true positive criteria’ for systematic reviews taken from the publication by [Shojania & Bero 2001][Shojania2001157162]. The publication is a diagnostic test accuracy assessment for the systematic review search strategy by the authors—the very search strategy that the search strategy of this review is based on. I quote:
  
> We regarded an article as a true positive only if the title or abstract explicitly identified the article as a systematic review or meta-analysis or if the article abstract indicated a strategy for locating the literature reviewed. Thus, an article that contained the phrase “literature review” in the title but merely stated that “relevant literature was reviewed” in the abstract would not count as a true positive. MEDLINE records without an abstract could be counted as true positives only if the title contained the words “meta-analysis,” “metaanalysis,” or “systematic review.”

There are special cases to be discussed:

* Practice guidelines as well as various review types that make use of systematic methodology (e.g., scoping reviews, realist syntheses, etc.) are not treated separately and are eligible only if the Shojania & Bero criteria are met. This is to ensure integrity of these criteria use.
* Commentaries, editorials, reprints, and other similar documents related to an original systematic review are included, as they reference a systematic review.
* Meta-analyses outside the context of a systematic review (variants on _data pooling_) are included if they are explicitly termed meta-analyses, as this is how the Shojania & Bero criteria are worded. See [Dickens 2020][Dickens2020emo0000905], for instance.
* Protocols for systematic reviews are included, as they reference a systematic review.
* Studies nesting a systematic review (e.g., case studies) are naturally included.

The record/study flow in this review is as follows:

* There is no instance when a record identified through search could be excluded from the review database, regardless of its eligibility status. However, ineligible records are not tagged and not appraised further. In contrast, all eligible records are amenable for tagging, selection, and crowdfunding process.
* Only those eligible records that have been selected or crowdfunded are subject to critical appraisal. Please find a description of the selection/crowdfunding process below.
* For all records that have been selected, all relevant reports are collected (either written by the same authors or by different authors, like in case of important referenced research). Reports are grouped into studies.
* Only for the studies appraised positively (ticked with `7. ✅ No Critical Conduct Flaws Identified by Zheln`, see below), practical implications are summarized and disseminated.

Selection/crowdfunding process:

* As a rule, I select a record for critical appraisal regardless of crowdfunding when I find the covered topic likely to have a very large practice impact with regard either to both global health care workers and the public or to the minorities. However, the decision to select is subjective and always mine to make.
* Language, date, peer-review status of the publication do not influence this decision. Also, content area of the publication (e.g., healthcare, ecology, chemistry, etc.) is also irrelevant to this decision.
* **COVID-19 publications are not selected, because there already are plenty of great resources, such as covid-nma.com, covidex.ai, cord19.vespa.ai,
WHO and Cochrane resources, just to name a few.
Also, I do not like the idea of changing universal focus of the project toward COVID-19.**
* Crowdfunding an appraisal of any eligible record is possible for any individual or organization. Such records are marked with different icons but are otherwise treated in the same way as the selected ones.

## 6 Information sources: Specify all databases, registers, websites, organisations, reference lists and other sources searched or consulted to identify studies. Specify the date when each source was last searched or consulted

[PRISMA-S checklist](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA-S_checklist.md)

## 7 Search strategy: Present the full search strategies for all databases, registers and websites, including any filters and limits used

[PRISMA-S checklist](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/PRISMA-S_checklist.md)

## 8 Selection process: Specify the methods used to decide whether a study met the inclusion criteria of the review, including how many reviewers screened each record and each report retrieved, whether they worked independently, and if applicable, details of automation tools used in the process

Please see the [5 Eligibility criteria](#5-eligibility-criteria-specify-the-inclusion-and-exclusion-criteria-for-the-review-and-how-studies-were-grouped-for-the-syntheses) section above.

Currently, I am the only person who undertakes the whole selection process. In the future, when more staff is available, other people will get involved; this will be additionally reported.

Likewise, no automation tools or crowdsourcing are currently used in the selection process. They are considered for future use; this will be additionally reported.

## 9 Data collection process: Specify the methods used to collect data from reports, including how many reviewers collected data from each report, whether they worked independently, any processes for obtaining or confirming data from study investigators, and if applicable, details of automation tools used in the process

Currently, I am the only person who undertakes the whole data collection process. In the future, when more staff is available, other people will get involved; this will be additionally reported.

Tagging of records:

* Specialty-tagging is done by the appraiser themselves based on whatever information they acquired during record screening.
* Specialty tags may also be assigned or removed throughout further full appraisal if this is deemed appropriate to better reflect the content of the record.
* The tags are chosen from [191 specialty tags](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/zheln_ama_specialty_tags.ls) made from a [list of 171 AMA Masterfile Physician Specialties](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/zheln_ama_specialty_tags_config.ls). I have chosen this specialty classification because it is, arguably, the most elaborate one (in terms of listed subspecialties) among those widely used. See the methods used to compile the lists of specialties and specialty tags in the [commit history of the Zheln methods repository](https://github.com/p1m-ortho/qs-global-ortho-search-queries/commits/global-sr-query).
* While tagging, the tagger should consider if the record would be accessible from its most relevant [specialty page](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/zheln_ama_specialty_page_filenames.ls) and ensure it would. Namely, the tagger must check the record against **each** specialty on the list, assess if this record could be of interest to a typical physician working in this specialty, and choose one specialty that looks most relevant in this regard.
* If choosing one such specialty is hard enough, choosing more than one is acceptable. As such, adding more than one specialty tag is actively discouraged unless absolutely unavoidable, as it remarkably compromises appraisal speed. Therefore, multiple specialties are warranted if and only if it is perceived to be faster to multiselect than choose one among several.
* I do recognize the best approach to specialty-grouping would be an evidence-based approach involving feedback from the physicians themselves. However, this is unavailable at the moment, so specialty-grouping will be theory-based. In the future, introducing empirical testing will be of interest.
* If the tagger is uncertain either of the article subject or any specialty scope, they should consult Google or other information sources until both seem perfectly clear.
* There is no limit to the number of specialty tags attached, but at least one tag should be chosen for each record. However, it became evident during pilot appraisals that some records on PubMed cover topics rather distant from health care. For these records, as an exception, there may be no specialty tags added when each and every specialty tag clearly does not apply.

* I do not plan to add any new specialty tags. However, if I hear about any changes to the AMA Masterfile Physician Specialties list, I will consider updating the Zheln specialty lists accordingly.

Collection of reports:

* I will try to collect the original full text and all other relevant reports associated with the record under review, if applicable, either using publicly available electronic resources or via private subscriptions or communication.
* If no full-text report is available for the record, I will put the `3. ❌ No Full Text Available to Zheln` label, place the record in the `awaiting crowdfunding` category, and reference it in the **Full Text Wanted** section of twice-weekly summary posts at Zheln.com. When new reports become available, I will go on with the critical appraisal.
* I will not routinely contact the authors or search extensively for additional reports, but if the study is somehow specifically important or additional appraisal is crowdfunded, will do that.

Data extraction from reports:

* Involves extracting the pragmatic outcomes that I deemed relevant for evidence-based practice. This is done only if the study has been positively appraised (ticked with `7. ✅ No Critical Conduct Flaws Identified by Zheln`). If so, I will extract the data directly in the text on the record page at Zheln.com.
* I will decide what outcomes to extract depending on the research question. As a rule, I will choose one among all outcomes mentioned in the study reports and will contrast it to the outcomes usually used in similar studies to decide if this is an acceptably practice-important and question-relevant outcome.
* To learn what outcomes are best suited for similar research questions, I am planning to use informal electronic search (Google, PubMed, etc.), that I am not planning to document, but I will summarize important information on this matter, including references, if applicable, in the text of the appraisal.
* No automation tools are currently used. If any are in the future, this will be additionally reported.
* Translation of reports written in languages other than English will usually be handled using [Google Translate](https://translate.google.com). In any way, this will be reported in the text on the record page at Zheln.com.
* Any inconsistencies met when collating multiple reports will be discussed and resolved narratively in the text on the record page at Zheln.com.
* If I did find an appropriate outcome in the study reports, I will use it as a _primary outcome_ to then assess effectiveness of the interventions. Otherwise, if I did not find any appropriate outcomes reported, I will mark the record as `4. ❌ Does Not Generate Pragmatic Evidence Directly Relevant to Evidence-Based Practice` and abstain from further appraisal.
* Eventually, I will formulate explicit practice-relevant statements based on the extracted health outcomes and quality-of-conduct assessment (see below). It will be done directly in the text on the record page at Zheln.com, stressed, and placed at its top.
* Also, the process of each critical appraisal is video-recorded and published on [YouTube](https://www.youtube.com/channel/UCMNQzA3-71TyD-fVbXnxfKQ) to disseminate this information among general audiences. One video-appraisal is uploaded [every working day](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/Work_Schedule.md).

## 10a Data items: List and define all outcomes for which data were sought. Specify whether all results that were compatible with each outcome domain in each study were sought(e.g. for all measures, time points, analyses), and if not, the methods used to decide which results to collect

Unfortunately, due to universal nature of the review, data items cannot be predefined. However, I will discuss all issues and report all decisions for each appraisal separately in the text on their record page at Zheln.com.

Please also see [9 Data collection process](#9-data-collection-process-specify-the-methods-used-to-collect-data-from-reports-including-how-many-reviewers-collected-data-from-each-report-whether-they-worked-independently-any-processes-for-obtaining-or-confirming-data-from-study-investigators-and-if-applicable-details-of-automation-tools-used-in-the-process).

## 10b Data items: List and define all other variables for which data were sought (e.g. participant and intervention characteristics, funding sources). Describe any assumptions made about any missing or unclear information

Please see [10a Data items](#10a-data-items-list-and-define-all-outcomes-for-which-data-were-sought-specify-whether-all-results-that-were-compatible-with-each-outcome-domain-in-each-study-were-soughteg-for-all-measures-time-points-analyses-and-if-not-the-methods-used-to-decide-which-results-to-collect).

## 11 Study risk of bias assessment: Specify the methods used to assess risk of bias in the included studies, including details of the tool(s) used, how many reviewers assessed each study and whether they worked independently, and if applicable, details of automation tools used in the process

Currently, I am the only person who undertakes the whole risk-of-bias assessment process. In the future, when more staff is available, other people will get involved; this will be additionally reported. Likewise, no automation tools are currently in use.

The methods of critical appraisal may be described in one phrase:
  
Informal study of the review documentation with replication of some of the elements of the review by a single appraiser to formulate their expert impression as to whether the review is reproducible and whether it is useful for evidence-based practice.

Please find by-step description below.

Assessing duplication:

* To find out, I am planning to use informal electronic search (Google, PubMed, etc.), that I am not planning to document, but I will summarize important information on this matter, including references, if applicable, in the text on the record page at Zheln.com.
* If I find evidence the study is duplicate (same population, same context, same interventions, same outcomes, with no reasonable reference to previous research), I should naturally not start answering the research question at this study but use previous research first. Therefore, I will abstain from appraising duplicate studies and will mark them as `5. ❌ Is Duplicate`.
* Otherwise, I will mark the study appropriately as `5. ✅ Not Found Duplicate by Zheln` and go on with the appraisal.

Replication:

* If a systematic review features practice-important outcomes _and_ does not look duplicate, it feels safe and appropriate to embark on its replication.
* In the course of this review, I am not planning to conduct exhaustive replications. In contrast, I am going to try and replicate selectively those review steps that look both easiest and most natural to redo.
* For example, rerunning PubMed/MEDLINE searches and replicating the review study set is usually simple enough and appealing, as is reproducing a random couple of data extraction forms. I will document these replication processes in the text on the record page at Zheln.com.
* If I deem the replication attempts more or less successful, I will label the study as `6. ✅ Passed Replication`.
* Otherwise, if the replication has largely failed, I will mark the study as `6. ❌ Failed Replication` and abstain from further appraisal because an irreproducible review is hardly systematic anymore.

Quality-of-conduct assessment:

* Replicability is a sign of sound conduct and good reporting but does not guarantee robustness of the review. Therefore, additional quality-of-conduct assessment is required.
* Some tools have been developed to assess risk of bias in systematic reviews, such as [ROBIS](https://www.bristol.ac.uk/population-health-sciences/projects/robis) or [CINeMA](https://doi.org/10.1371/journal.pmed.1003082). However, they are rather recent, and there is evidence agreement is low at least for some of them ([Gates 2020][Gates2020915]).
* In contrast, [MECIR](https://community.cochrane.org/mecir-manual) have been out there for quite some time now. Also, it has been neatly integrated into [Cochrane Handbook 6](https://training.cochrane.org/handbook/current) that provides further insight into these issues.
* Thus, I elect to use the [MECIR conduct standards](https://community.cochrane.org/mecir-manual/standards-conduct-new-cochrane-intervention-reviews-c1-c75) to assess quality of conduct. I will go over all 75 MECIR conduct items to get an understanding and will document for each item if it was followed, in my view. I will also provide rationale where relevant. All the documentation will take place directly in the text on the record page at Zheln.com.
* In general, I expect _mandatory_ MECIR items to be followed, whereas _highly desirable_ items may be ignored. However, I acknowledge that MECIR standards (1) are not absolute and (2) were developed for intervention reviews only (whereas this overview may feature other systematic reviews as well). Also, some of the items would look critical for one review and not critical for another.
* Hence, the final decision about whether or not I have observed evidence of critical conduct flaws is always mine to make; at the same time, I will do my best to accurately substantiate my findings in the appraisal text. My decision may be `7. ❌ Has Critical Conduct Flaws`, `7. ✅ No Critical Conduct Flaws Identified by Zheln`, or `7. ✅ No Conduct Flaws Identified by Zheln`, depending on those findings.

I am not planning to conduct any assessment of quality of reporting.

Also, I will note this as a solely subjective item: `8. 👍 Liked by Zheln` if I find the review useful overall and `8. 👎 Disliked by Zheln` if I find otherwise. I will provide any personal comment if my decision did not seem obvious.

## 12 Effect measures: Specify for each outcome the effect measure(s) (e.g. risk ratio, mean difference) used in the synthesis or presentation of results

No across-studies synthesis is planned at the moment. If any is conducted, it will be reported separately for each study in the text on the record page at Zheln.com.

## 13a Synthesis methods: Describe the processes used to decide which studies were eligible for each synthesis

Please see [12 Effect measures](#12-effect-measures-specify-for-each-outcome-the-effect-measures-eg-risk-ratio-mean-difference-used-in-the-synthesis-or-presentation-of-results).

## 13b Synthesis methods: Describe any methods required to prepare the data for presentation or synthesis, such as handling of missing summary statistics, or data conversions

Please see [12 Effect measures](#12-effect-measures-specify-for-each-outcome-the-effect-measures-eg-risk-ratio-mean-difference-used-in-the-synthesis-or-presentation-of-results).

## 13c Synthesis methods: Describe any methods used to tabulate or visually display results of individual studies and syntheses

Please see [12 Effect measures](#12-effect-measures-specify-for-each-outcome-the-effect-measures-eg-risk-ratio-mean-difference-used-in-the-synthesis-or-presentation-of-results).

## 13d Synthesis methods: Describe anymethods used to synthesize results and provide a rationale for the choice(s). If meta-analysis was performed, describe the model(s), method(s) to identify the presence and extent of statistical heterogeneity, and software package(s) used

Please see [12 Effect measures](#12-effect-measures-specify-for-each-outcome-the-effect-measures-eg-risk-ratio-mean-difference-used-in-the-synthesis-or-presentation-of-results).

## 13e Synthesis methods: Describe any methods used to explore possible causes of heterogeneity among study results

Please see [12 Effect measures](#12-effect-measures-specify-for-each-outcome-the-effect-measures-eg-risk-ratio-mean-difference-used-in-the-synthesis-or-presentation-of-results).

## 13f Synthesis methods: Describe any sensitivity analyses conducted to assess robustness of the synthesized results

Please see [12 Effect measures](#12-effect-measures-specify-for-each-outcome-the-effect-measures-eg-risk-ratio-mean-difference-used-in-the-synthesis-or-presentation-of-results).

## 14 Reporting bias assessment: Describe any methods used to assess risk of bias due to missing results in a synthesis (arising from reporting biases)

* I will use the preliminary version of the [ROB-ME](https://www.riskofbias.info/welcome/rob-me-tool) tool. All documentation will take place directly in the text on the record page at Zheln.com. However, I do not have in mind any generic algorithm to assess overall risk of bias, so I will elaborate on it for each appraisal separately.
* I will be the only assessor for now.
* No automation tools will be used.
* The authors will not be contacted routinely. However, if specifically requested when crowdfunded or at will, will be done.

## 15 Certainty assessment: Describe any methods used to assess certainty (or confidence) in the body of evidence for an outcome

* I will use the [GRADE](https://gdt.gradepro.org/app/handbook/handbook.html) tool in its interpretation for systematic review authors. All documentation will take place directly in the text on the record page at Zheln.com. I do not have in mind any generic algorithm to assess certainty of evidence, so I will elaborate on it for each appraisal separately.
* I will be the only assessor for now.
* No automation tools will be used.
* The authors will not be contacted routinely. However, if specifically requested when crowdfunded or at will, will be done.

# RESULTS

## 16a Study selection: Describe the results of the search and selection process, from the number of records identified in the search to the number of studies included in the review, ideally using a flow diagram (see Figure 1)

* [Living appraisal log](https://github.com/p1m-ortho/qs-global-ortho-search-queries/blob/global-sr-query/zheln/Appraisal_Log.md) (Table).
* [Living PRISMA 2020 flow diagrams](https://github.com/drzhelnov/zheln.github.io/tree/master/flow-diagram) (an updated diagram image is uploaded every Wednesday and Saturday).

## 16b Study selection: Cite studies that met many but not all inclusion criteria (‘near-misses’) and explain why they were excluded



## 17 Study characteristics: Cite each included study and present its characteristics



## 18 Risk of bias in studies: Present assessments of risk of bias for each included study



## 19 Results of individual studies: For all outcomes, present, for each study: (a) summary statistics for each group (where appropriate) and (b) an effect estimate andits precision (e.g. confidence/credible interval), ideally using structured tables or plots



## 20a Results of syntheses: For each synthesis, briefly summarise the characteristics and risk of bias among contributing studies



## 20b Results of syntheses: Present results of all statistical syntheses conducted. If meta-analysis was done, present for each the summary estimate and its precision (e.g. confidence/credible interval) and measures of statistical heterogeneity. If comparing groups, describe the direction of the effect



## 20c Results of syntheses: Present results of all investigations of possible causes of heterogeneity among study results



## 20d Results of syntheses: Present results of all sensitivity analyses conducted to assess the robustness of the synthesized results



## 21 Reporting biases: Present assessments of risk of bias due to missing results(arising from reporting biases) for each synthesis assessed



## 22 Certainty of evidence: Present assessments of certainty (or confidence) in the body of evidence for each outcome assessed



# DISCUSSION

## 23a Discussion: Provide a general interpretation of the results in the context of other evidence



## 23b Discussion: Discuss any limitations of the evidence included in the review



## 23c Discussion: Discuss any limitations of the review processes used



## 23d Discussion: Discuss implications of the results for practice, policy, and future research



# OTHER INFORMATION

## 24a Registration and protocol: Provide registration information for the review, including register name and registration number, or state that the review was not registered

The project is hosted on GitHub. Also, there is an [umbrella Open Science Framework project](https://doi.org/10.17605/OSF.IO/EJKFC) that mediates publication of Zheln methods between raw GitHub data (the [primary](https://github.com/drzhelnov/zheln.github.io) and [methods](https://github.com/p1m-ortho/qs-global-ortho-search-queries/tree/global-sr-query) repositories) and stable Open Science Framework preprints (project components, such as the [replicated PubMed systematic subset](https://doi.org/10.17605/OSF.IO/Z3JU7), [systematic review protocol](https://doi.org/10.17605/OSF.IO/K4Y83), [living systematic review report](https://doi.org/10.17605/OSF.IO/G6RA9)).

Operational information on how the [Zheln website](https://zheln.com) works and how to recreate the research environment in use are also available from these repositories.

The protocol for this overview of systematic reviews is about to be registered in [PROSPERO](https://www.crd.york.ac.uk/prospero/). Any amendments to the protocol will be available from the above-mentioned repositories and will be reported in summary posts at Zheln.com.

## 24b Registration and protocol: Indicate where the review protocol can be accessed, or state that a protocol was not prepared

Please see [24a Registration and protocol](#24a-registration-and-protocol-provide-registration-information-for-the-review-including-register-name-and-registration-number-or-state-that-the-review-was-not-registered).

## 24c Registration and protocol: Describe and explain any amendments to information provided at registration or in the protocol

Please see [24a Registration and protocol](#24a-registration-and-protocol-provide-registration-information-for-the-review-including-register-name-and-registration-number-or-state-that-the-review-was-not-registered).

## 25 Support: Describe sources of financial or non-financial support for the review, and the role of the funders or sponsors in the review

* There is no other funding for Zheln besides my private devotion and crowdfunding.
* Crowdfunding details are available from the [Zheln website](https://zheln.com). Crowd funders had no role in the design of the protocol. They will be able to request critical appraisal and additional critical appraisal (with new data provided) of any eligible record but will not influence the review process otherwise.
* If Zheln videos get popular enough in the future, will also monetize these using the [YouTube Partner Program](https://support.google.com/youtube/answer/72857).

## 26 Competing interests: Declare any competing interests of review authors

I, Pavel Zhelnov ([ORCID 0000-0003-2767-5123](http://orcid.org/0000-0003-2767-5123)), am the only author so far and declare no competing interests.

## 27 Availability of data, code and other materials: Report which of the following are publicly available and where they can be found: template data collection forms; data extracted from included studies; data used for all analyses; analytic code; any other materials used in the review

Please see [24a Registration and protocol](#24a-registration-and-protocol-provide-registration-information-for-the-review-including-register-name-and-registration-number-or-state-that-the-review-was-not-registered).

In addition to all review data being publicly available, the project management information is also publicly available:

* Zheln uses public [GitHub kanban boards](https://docs.github.com/en/github/managing-your-work-on-github/about-project-boards) to manage its workflow.
* Namely, important tasks needed to accomplish the [Zheln’s mission](#zhelns-mission) are created as public Projects on the [Zheln GitHub page](https://github.com/drzhelnov/zheln.github.io/projects), e.g., [Streamline Zheln](https://github.com/drzhelnov/zheln.github.io/projects/1) or [Register with PROSPERO](https://github.com/drzhelnov/zheln.github.io/projects/2).
* Further, these tasks are broken down into smaller blocks ([GitHub Issues](https://github.com/drzhelnov/zheln.github.io/issues)) and are displayed on the projects’ kanban boards.
* Finally, each week, short-term [GitHub Milestones](https://github.com/drzhelnov/zheln.github.io/milestones) are created and then assigned to the Issues to track weekly progress.
* All of these elements are public. Issues are open for public comment (GitHub registration needed).
* To maximize publicity, Zheln additionally endeavors to publish [summary posts](https://zheln.com) twice a week, where I overview the events that happened on Zheln since the last summary and that I consider important.

[Page2020gwdhk]: https://doi.org/10.31222/osf.io/gwdhk "Page MJ, Moher D, Bossuyt P, Boutron I, Hoffmann T, Mulrow CD, Shamseer L, Tetzlaff J, Akl E, Brennan SE, Chou R, Glanville J, Grimshaw J, Hróbjartsson A, Lalu MM, Li T, Loder E, Mayo-Wilson E, McDonald S, McGuinness LA, Stewart L, Thomas J, Tricco A, Welch VA, Whiting P, McKenzie J. PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. MetaArXiv. 2020 Sep 14. doi: 10.31222/osf.io/gwdhk"

[Papes2020e13301]: https://doi.org/10.1111/eci.13301 "Papes D, Jeroncic A, Ozimec E. Redundancy and methodological issues in articles on COVID-19. Eur J Clin Invest. 2020 Jun 7:e13301. doi: 10.1111/eci.13301. Epub ahead of print. PMID: 32506512; PMCID: PMC7300618."

[Bastian2010e1000326]: https://doi.org/10.1371/journal.pmed.1000326 "Bastian H, Glasziou P, Chalmers I. Seventy-five trials and eleven systematic reviews a day: how will we ever keep up? PLoS Med. 2010 Sep 21;7(9):e1000326. doi: 10.1371/journal.pmed.1000326. PMID: 20877712; PMCID: PMC2943439."

[Contou202070]: https://doi.org/10.1186/s13613-020-00688-2 "Contou D, Thirion M, Pajot O, Plantefève G, Mentec H. Journal club in an ICU: rate and factors associated with practice-changing articles. Analysis of 1712 articles read over a 13-year period (2007-2019). Ann Intensive Care. 2020 Jun 1;10(1):70. doi: 10.1186/s13613-020-00688-2. PMID: 32488529; PMCID: PMC7266895."

[Bougioukas2020e12318]: https://doi.org/10.1111/hir.12318 "Bougioukas KI, Bouras EC, Avgerinos KI, Dardavessis T, Haidich AB. How to keep up to date with medical information using web-based resources: a systematised review and narrative synthesis. Health Info Libr J. 2020 Jul 21. doi: 10.1111/hir.12318. Epub ahead of print. PMID: 32691960."

[Boutron2020135142]: https://doi.org/10.1016/j.jclinepi.2020.01.024 "Boutron I, Créquit P, Williams H, Meerpohl J, Craig JC, Ravaud P. Future of evidence ecosystem series: 1. Introduction Evidence synthesis ecosystem needs dramatic change. J Clin Epidemiol. 2020 Jul;123:135-142. doi: 10.1016/j.jclinepi.2020.01.024. Epub 2020 Mar 4. PMID: 32145367."

[Haynes2006162164]: https://doi.org/10.1136/ebm.11.6.162-a "Haynes RB. Of studies, syntheses, synopses, summaries, and systems: the “5S” evolution of information services for evidence-based health care decisions. ACP J Club. 2006 Nov-Dec;145(3):A8. doi: 10.1136/ebm.11.6.162-a. PMID: 17080967."

[Chandran2020147154]: https://doi.org/10.7324/JAPS.2020.10717 "Chandran PV, Khan S, Pai KG, Khera K, Devi ES, Athira B, Thunga G. Evidence-based medicine databases: an overview. J Appl Pharm Sci. 2020 Jul;10(7):147-154. doi: 10.7324/JAPS.2020.10717."

[Booth2019e001107]: https://doi.org/10.1136/bmjgh-2018-001107 "Booth A, Noyes J, Flemming K, Moore G, Tunçalp Ö, Shakibazadeh E. Formulating questions to explore complex interventions within qualitative evidence synthesis. BMJ Glob Health. 2019 Jan 25;4(Suppl 1):e001107. doi: 10.1136/bmjgh-2018-001107. PMID: 30775019; PMCID: PMC6350737."

[Shojania2001157162]: https://www.researchgate.net/publication/11820967_Taking_Advantage_of_the_Explosion_of_Systematic_Reviews_An_Efficient_MEDLINE_Search_Strategy "Shojania KG, Bero LA. Taking advantage of the explosion of systematic reviews: an efficient MEDLINE search strategy. Eff Clin Pract. 2001 Jul-Aug;4(4):157-62. PMID: 11525102."

[Dickens2020emo0000905]: https://doi.org/10.1037/emo0000905 "Dickens LR, Robins RW. Pride: A meta-analytic project. Emotion. 2020 Nov 12. doi: 10.1037/emo0000905. Epub ahead of print. PMID: 33180528."

[Gates2020915]: https://doi.org/10.1016/j.jclinepi.2020.04.026 "Gates M, Gates A, Duarte G, Cary M, Becker M, Prediger B, Vandermeer B, Fernandes RM, Pieper D, Hartling L. Quality and risk of bias appraisals of systematic reviews are inconsistent across reviewers and centers. J Clin Epidemiol. 2020 Sep;125:9-15. doi: 10.1016/j.jclinepi.2020.04.026. Epub 2020 May 19. PMID: 32416337."
